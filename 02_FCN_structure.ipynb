{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 진행계획\n",
    "    - FCN 방식을 통해서 스펙트로그램의 어떤 부분이 그 단어를 의미하는지 표시하는 것에 집중\n",
    "    - 일단 label이 bed 인것만 끄집어내서 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/wkentaro/pytorch-fcn/blob/master/torchfcn/models/fcn32s.py <- 여기서 들고옴\n",
    "# https://github.com/shelhamer/fcn.berkeleyvision.org/blob/master/surgery.py <- 이건 윗사람이 들고온곳인듯\n",
    "\n",
    "def get_upsampling_weight(in_channels, out_channels, kernel_size):\n",
    "    \"\"\"Make a 2D bilinear kernel suitable for upsampling\"\"\"\n",
    "    factor = (kernel_size + 1) // 2\n",
    "    if kernel_size % 2 == 1:\n",
    "        center = factor - 1\n",
    "    else:\n",
    "        center = factor - 0.5\n",
    "    og = np.ogrid[:kernel_size, :kernel_size]\n",
    "    filt = (1 - abs(og[0] - center) / factor) * \\\n",
    "           (1 - abs(og[1] - center) / factor)\n",
    "    weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size),\n",
    "                      dtype=np.float64)\n",
    "    weight[range(in_channels), range(out_channels), :, :] = filt\n",
    "    return torch.from_numpy(weight).float()\n",
    "\n",
    "\n",
    "class FCN32s(nn.Module):\n",
    "    def __init__(self, n_class=2):\n",
    "        super(FCN8s, self).__init__()\n",
    "        # conv1\n",
    "        self.conv1_1 = nn.Conv2d(1, 64, 3, padding=100)\n",
    "        self.relu1_1 = nn.ReLU(inplace=True)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.relu1_2 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/2\n",
    "\n",
    "        # conv2\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.relu2_1 = nn.ReLU(inplace=True)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.relu2_2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/4\n",
    "\n",
    "        # conv3\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.relu3_1 = nn.ReLU(inplace=True)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.relu3_2 = nn.ReLU(inplace=True)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.relu3_3 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/8\n",
    "\n",
    "        # conv4\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.relu4_1 = nn.ReLU(inplace=True)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.relu4_2 = nn.ReLU(inplace=True)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.relu4_3 = nn.ReLU(inplace=True)\n",
    "        self.pool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/16\n",
    "\n",
    "        # conv5\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.relu5_1 = nn.ReLU(inplace=True)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.relu5_2 = nn.ReLU(inplace=True)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.relu5_3 = nn.ReLU(inplace=True)\n",
    "        self.pool5 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/32\n",
    "\n",
    "        # fc6\n",
    "        self.fc6 = nn.Conv2d(512, 4096, 7)\n",
    "        self.relu6 = nn.ReLU(inplace=True)\n",
    "        self.drop6 = nn.Dropout2d()\n",
    "\n",
    "        # fc7\n",
    "        self.fc7 = nn.Conv2d(4096, 4096, 1)\n",
    "        self.relu7 = nn.ReLU(inplace=True)\n",
    "        self.drop7 = nn.Dropout2d()\n",
    "\n",
    "        self.score_fr = nn.Conv2d(4096, n_class, 1)\n",
    "        self.score_pool3 = nn.Conv2d(256, n_class, 1)\n",
    "        self.score_pool4 = nn.Conv2d(512, n_class, 1)\n",
    "\n",
    "        self.upscore2 = nn.ConvTranspose2d(\n",
    "            n_class, n_class, 4, stride=2, bias=False)\n",
    "        self.upscore8 = nn.ConvTranspose2d(\n",
    "            n_class, n_class, 16, stride=8, bias=False)\n",
    "        self.upscore_pool4 = nn.ConvTranspose2d(\n",
    "            n_class, n_class, 4, stride=2, bias=False)\n",
    "\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                m.weight.data.zero_()\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            if isinstance(m, nn.ConvTranspose2d):\n",
    "                assert m.kernel_size[0] == m.kernel_size[1]\n",
    "                initial_weight = get_upsampling_weight(\n",
    "                    m.in_channels, m.out_channels, m.kernel_size[0])\n",
    "                m.weight.data.copy_(initial_weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        h = self.relu1_1(self.conv1_1(h))\n",
    "        h = self.relu1_2(self.conv1_2(h))\n",
    "        h = self.pool1(h)\n",
    "\n",
    "        h = self.relu2_1(self.conv2_1(h))\n",
    "        h = self.relu2_2(self.conv2_2(h))\n",
    "        h = self.pool2(h)\n",
    "\n",
    "        h = self.relu3_1(self.conv3_1(h))\n",
    "        h = self.relu3_2(self.conv3_2(h))\n",
    "        h = self.relu3_3(self.conv3_3(h))\n",
    "        h = self.pool3(h)\n",
    "        pool3 = h  # 1/8\n",
    "\n",
    "        h = self.relu4_1(self.conv4_1(h))\n",
    "        h = self.relu4_2(self.conv4_2(h))\n",
    "        h = self.relu4_3(self.conv4_3(h))\n",
    "        h = self.pool4(h)\n",
    "        pool4 = h  # 1/16\n",
    "\n",
    "        h = self.relu5_1(self.conv5_1(h))\n",
    "        h = self.relu5_2(self.conv5_2(h))\n",
    "        h = self.relu5_3(self.conv5_3(h))\n",
    "        h = self.pool5(h)\n",
    "\n",
    "        h = self.relu6(self.fc6(h))\n",
    "        h = self.drop6(h)\n",
    "\n",
    "        h = self.relu7(self.fc7(h))\n",
    "        h = self.drop7(h)\n",
    "\n",
    "        h = self.score_fr(h)\n",
    "        h = self.upscore2(h)\n",
    "        upscore2 = h  # 1/16\n",
    "\n",
    "        h = self.score_pool4(pool4)\n",
    "        h = h[:, :, 5:5 + upscore2.size()[2], 5:5 + upscore2.size()[3]]\n",
    "        score_pool4c = h  # 1/16\n",
    "\n",
    "        h = upscore2 + score_pool4c  # 1/16\n",
    "        h = self.upscore_pool4(h)\n",
    "        upscore_pool4 = h  # 1/8\n",
    "\n",
    "        h = self.score_pool3(pool3)\n",
    "        h = h[:, :,\n",
    "              9:9 + upscore_pool4.size()[2],\n",
    "              9:9 + upscore_pool4.size()[3]]\n",
    "        score_pool3c = h  # 1/8\n",
    "\n",
    "        h = upscore_pool4 + score_pool3c  # 1/8\n",
    "\n",
    "        h = self.upscore8(h)\n",
    "        h = h[:, :, 31:31 + x.size()[2], 31:31 + x.size()[3]].contiguous()\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy2d(input, target, weight=None, size_average=True):\n",
    "    # input: (n, c, h, w), target: (n, h, w)\n",
    "    n, c, h, w = input.size()\n",
    "    # log_p: (n, c, h, w)\n",
    "    if LooseVersion(torch.__version__) < LooseVersion('0.3'):\n",
    "        # ==0.2.X\n",
    "        log_p = F.log_softmax(input)\n",
    "    else:\n",
    "        # >=0.3\n",
    "        log_p = F.log_softmax(input, dim=1)\n",
    "    # log_p: (n*h*w, c)\n",
    "    log_p = log_p.transpose(1, 2).transpose(2, 3).contiguous()\n",
    "    log_p = log_p[target.view(n, h, w, 1).repeat(1, 1, 1, c) >= 0]\n",
    "    log_p = log_p.view(-1, c)\n",
    "    # target: (n*h*w,)\n",
    "    mask = target >= 0\n",
    "    target = target[mask]\n",
    "    loss = F.nll_loss(log_p, target, weight=weight, reduction='sum')\n",
    "    if size_average:\n",
    "        loss /= mask.data.sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from distutils.version import LooseVersion\n",
    "import math\n",
    "import os\n",
    "import os.path as osp\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "#import skimage.io\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "#import tqdm\n",
    "\n",
    "#import torchfcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "\n",
    "    def __init__(self, cuda, model, optimizer,\n",
    "                 train_loader, val_loader, out, max_iter,\n",
    "                 size_average=False, interval_validate=None):\n",
    "        self.cuda = cuda\n",
    "\n",
    "        self.model = model\n",
    "        self.optim = optimizer\n",
    "\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "        self.timestamp_start = \\\n",
    "            datetime.datetime.now()\n",
    "        self.size_average = size_average\n",
    "\n",
    "        if interval_validate is None:\n",
    "            self.interval_validate = len(self.train_loader)\n",
    "        else:\n",
    "            self.interval_validate = interval_validate\n",
    "\n",
    "        self.out = out\n",
    "        if not osp.exists(self.out):\n",
    "            os.makedirs(self.out)\n",
    "\n",
    "        self.log_headers = [\n",
    "            'epoch',\n",
    "            'iteration',\n",
    "            'train/loss',\n",
    "            'train/acc',\n",
    "            'train/acc_cls',\n",
    "            'train/mean_iu',\n",
    "            'train/fwavacc',\n",
    "            'valid/loss',\n",
    "            'valid/acc',\n",
    "            'valid/acc_cls',\n",
    "            'valid/mean_iu',\n",
    "            'valid/fwavacc',\n",
    "            'elapsed_time',\n",
    "        ]\n",
    "        if not osp.exists(osp.join(self.out, 'log.csv')):\n",
    "            with open(osp.join(self.out, 'log.csv'), 'w') as f:\n",
    "                f.write(','.join(self.log_headers) + '\\n')\n",
    "\n",
    "        self.epoch = 0\n",
    "        self.iteration = 0\n",
    "        self.max_iter = max_iter\n",
    "        self.best_mean_iu = 0\n",
    "\n",
    "    def validate(self):\n",
    "        training = self.model.training\n",
    "        self.model.eval()\n",
    "\n",
    "        n_class = len(self.val_loader.dataset.class_names)\n",
    "\n",
    "        val_loss = 0\n",
    "        visualizations = []\n",
    "        label_trues, label_preds = [], []\n",
    "        for batch_idx, (data, target) in tqdm.tqdm(\n",
    "                enumerate(self.val_loader), total=len(self.val_loader),\n",
    "                desc='Valid iteration=%d' % self.iteration, ncols=80,\n",
    "                leave=False):\n",
    "            if self.cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            with torch.no_grad():\n",
    "                score = self.model(data)\n",
    "\n",
    "            loss = cross_entropy2d(score, target,\n",
    "                                   size_average=self.size_average)\n",
    "            loss_data = loss.data.item()\n",
    "            if np.isnan(loss_data):\n",
    "                raise ValueError('loss is nan while validating')\n",
    "            val_loss += loss_data / len(data)\n",
    "\n",
    "            imgs = data.data.cpu()\n",
    "            lbl_pred = score.data.max(1)[1].cpu().numpy()[:, :, :]\n",
    "            lbl_true = target.data.cpu()\n",
    "            for img, lt, lp in zip(imgs, lbl_true, lbl_pred):\n",
    "                img, lt = self.val_loader.dataset.untransform(img, lt)\n",
    "                label_trues.append(lt)\n",
    "                label_preds.append(lp)\n",
    "                if len(visualizations) < 9:\n",
    "                    viz = fcn.utils.visualize_segmentation(\n",
    "                        lbl_pred=lp, lbl_true=lt, img=img, n_class=n_class)\n",
    "                    visualizations.append(viz)\n",
    "        metrics = torchfcn.utils.label_accuracy_score(\n",
    "            label_trues, label_preds, n_class)\n",
    "\n",
    "        out = osp.join(self.out, 'visualization_viz')\n",
    "        if not osp.exists(out):\n",
    "            os.makedirs(out)\n",
    "        out_file = osp.join(out, 'iter%012d.jpg' % self.iteration)\n",
    "        skimage.io.imsave(out_file, fcn.utils.get_tile_image(visualizations))\n",
    "\n",
    "        val_loss /= len(self.val_loader)\n",
    "\n",
    "        with open(osp.join(self.out, 'log.csv'), 'a') as f:\n",
    "            elapsed_time = (\n",
    "                datetime.datetime.now(pytz.timezone('Asia/Tokyo')) -\n",
    "                self.timestamp_start).total_seconds()\n",
    "            log = [self.epoch, self.iteration] + [''] * 5 + \\\n",
    "                  [val_loss] + list(metrics) + [elapsed_time]\n",
    "            log = map(str, log)\n",
    "            f.write(','.join(log) + '\\n')\n",
    "\n",
    "        mean_iu = metrics[2]\n",
    "        is_best = mean_iu > self.best_mean_iu\n",
    "        if is_best:\n",
    "            self.best_mean_iu = mean_iu\n",
    "        torch.save({\n",
    "            'epoch': self.epoch,\n",
    "            'iteration': self.iteration,\n",
    "            'arch': self.model.__class__.__name__,\n",
    "            'optim_state_dict': self.optim.state_dict(),\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'best_mean_iu': self.best_mean_iu,\n",
    "        }, osp.join(self.out, 'checkpoint.pth.tar'))\n",
    "        if is_best:\n",
    "            shutil.copy(osp.join(self.out, 'checkpoint.pth.tar'),\n",
    "                        osp.join(self.out, 'model_best.pth.tar'))\n",
    "\n",
    "        if training:\n",
    "            self.model.train()\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "\n",
    "        n_class = len(self.train_loader.dataset.class_names)\n",
    "\n",
    "        for batch_idx, (data, target) in tqdm.tqdm(\n",
    "                enumerate(self.train_loader), total=len(self.train_loader),\n",
    "                desc='Train epoch=%d' % self.epoch, ncols=80, leave=False):\n",
    "            iteration = batch_idx + self.epoch * len(self.train_loader)\n",
    "            if self.iteration != 0 and (iteration - 1) != self.iteration:\n",
    "                continue  # for resuming\n",
    "            self.iteration = iteration\n",
    "\n",
    "            if self.iteration % self.interval_validate == 0:\n",
    "                self.validate()\n",
    "\n",
    "            assert self.model.training\n",
    "\n",
    "            if self.cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            self.optim.zero_grad()\n",
    "            score = self.model(data)\n",
    "\n",
    "            loss = cross_entropy2d(score, target,\n",
    "                                   size_average=self.size_average)\n",
    "            loss /= len(data)\n",
    "            loss_data = loss.data.item()\n",
    "            if np.isnan(loss_data):\n",
    "                raise ValueError('loss is nan while training')\n",
    "            loss.backward()\n",
    "            self.optim.step()\n",
    "\n",
    "            metrics = []\n",
    "            lbl_pred = score.data.max(1)[1].cpu().numpy()[:, :, :]\n",
    "            lbl_true = target.data.cpu().numpy()\n",
    "            acc, acc_cls, mean_iu, fwavacc = \\\n",
    "                torchfcn.utils.label_accuracy_score(\n",
    "                    lbl_true, lbl_pred, n_class=n_class)\n",
    "            metrics.append((acc, acc_cls, mean_iu, fwavacc))\n",
    "            metrics = np.mean(metrics, axis=0)\n",
    "\n",
    "            with open(osp.join(self.out, 'log.csv'), 'a') as f:\n",
    "                elapsed_time = (\n",
    "                    datetime.datetime.now(pytz.timezone('Asia/Tokyo')) -\n",
    "                    self.timestamp_start).total_seconds()\n",
    "                log = [self.epoch, self.iteration] + [loss_data] + \\\n",
    "                    metrics.tolist() + [''] * 5 + [elapsed_time]\n",
    "                log = map(str, log)\n",
    "                f.write(','.join(log) + '\\n')\n",
    "\n",
    "            if self.iteration >= self.max_iter:\n",
    "                break\n",
    "\n",
    "    def train(self):\n",
    "        max_epoch = int(math.ceil(1. * self.max_iter / len(self.train_loader)))\n",
    "        for epoch in tqdm.trange(self.epoch, max_epoch,\n",
    "                                 desc='Train', ncols=80):\n",
    "            self.epoch = epoch\n",
    "            self.train_epoch()\n",
    "            if self.iteration >= self.max_iter:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictToObject(object):\n",
    "\n",
    "    def __init__(self, dictionary):\n",
    "        def _traverse(key, element):\n",
    "            if isinstance(element, dict):\n",
    "                return key, DictToObject(element)\n",
    "            else:\n",
    "                return key, element\n",
    "\n",
    "        objd = dict(_traverse(k, v) for k, v in dictionary.items())\n",
    "        self.__dict__.update(objd)\n",
    "\n",
    "test_dict = {'a' : 3, 'b' : 4}\n",
    "\n",
    "args = DictToObject(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.add_argument('-g', '--gpu', type=int, required=True, help='gpu id')\n",
    "    parser.add_argument('--resume', help='checkpoint path')\n",
    "    # configurations (same configuration as original work)\n",
    "    # https://github.com/shelhamer/fcn.berkeleyvision.org\n",
    "    parser.add_argument(\n",
    "        '--max-iteration', type=int, default=100000, help='max iteration'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--lr', type=float, default=1.0e-14, help='learning rate',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--weight-decay', type=float, default=0.0005, help='weight decay',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--momentum', type=float, default=0.99, help='momentum',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--pretrained-model',\n",
    "        default=torchfcn.models.FCN16s.download(),\n",
    "        help='pretrained model of FCN16s',\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    args.model = 'FCN8s'\n",
    "    args.git_hash = git_hash()\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    args.out = osp.join(here, 'logs', now.strftime('%Y%m%d_%H%M%S.%f'))\n",
    "\n",
    "    os.makedirs(args.out)\n",
    "    with open(osp.join(args.out, 'config.yaml'), 'w') as f:\n",
    "        yaml.safe_dump(args.__dict__, f, default_flow_style=False)\n",
    "\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(args.gpu)\n",
    "    cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchfcn.models.FCN8s(n_class=21)\n",
    "start_epoch = 0\n",
    "start_iteration = 0\n",
    "if args.resume:\n",
    "    checkpoint = torch.load(args.resume)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    start_iteration = checkpoint['iteration']\n",
    "else:\n",
    "    fcn16s = torchfcn.models.FCN16s()\n",
    "    state_dict = torch.load(args.pretrained_model)\n",
    "    try:\n",
    "        fcn16s.load_state_dict(state_dict)\n",
    "    except RuntimeError:\n",
    "        fcn16s.load_state_dict(state_dict['model_state_dict'])\n",
    "    model.copy_params_from_fcn16s(fcn16s)\n",
    "if cuda:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(\n",
    "    [\n",
    "        {'params': get_parameters(model, bias=False)},\n",
    "        {'params': get_parameters(model, bias=True),\n",
    "         'lr': args.lr * 2, 'weight_decay': 0},\n",
    "    ],\n",
    "    lr=args.lr,\n",
    "    momentum=args.momentum,\n",
    "    weight_decay=args.weight_decay)\n",
    "if args.resume:\n",
    "    optim.load_state_dict(checkpoint['optim_state_dict'])\n",
    "\n",
    "trainer = torchfcn.Trainer(\n",
    "    cuda=cuda,\n",
    "    model=model,\n",
    "    optimizer=optim,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    out=args.out,\n",
    "    max_iter=args.max_iteration,\n",
    "    interval_validate=4000,\n",
    ")\n",
    "trainer.epoch = start_epoch\n",
    "trainer.iteration = start_iteration\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
